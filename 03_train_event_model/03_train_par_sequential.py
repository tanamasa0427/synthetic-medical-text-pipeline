# ==========================================
# âœ… PARSynthesizer (æ™‚ç³»åˆ—) + è»½é‡ç‰ˆ
# (Kaggle /working/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œ)
# [2025-10-23 ä¿®æ­£ç‰ˆï¼šãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®šæ–¹æ³•ã‚’ä¿®æ­£]
# ==========================================
import os
import pandas as pd
import numpy as np
import torch
from datetime import datetime
from tqdm import tqdm
from sdv.sequential import PARSynthesizer
# ------------------------------------------
# âš ï¸ ä¿®æ­£ç‚¹ï¼š SingleTableMetadata ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ------------------------------------------
from sdv.metadata import SingleTableMetadata

# ------------------------------------------
# âš™ï¸ å®Ÿè¡Œç’°å¢ƒã«åˆã‚ã›ã¦ã€ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’è¨­å®š
# ------------------------------------------
ENVIRONMENT = "kaggle_working" 

if ENVIRONMENT == "kaggle_working":
    BASE_DIR = "/kaggle/working/synthetic-medical-text-pipeline"
    INPUT_DIR = os.path.join(BASE_DIR, "data/inputs")
    OUTPUT_DIR = os.path.join(BASE_DIR, "data/outputs")
elif ENVIRONMENT == "colab":
    BASE_DIR = "/content/synthetic-medical-text-pipeline"
    INPUT_DIR = os.path.join(BASE_DIR, "data/inputs")
    OUTPUT_DIR = os.path.join(BASE_DIR, "data/outputs")
else:
    INPUT_DIR = "./data/inputs"
    OUTPUT_DIR = "./data/outputs"

# ------------------------------------------

os.makedirs(OUTPUT_DIR, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
print(f"ğŸ“¦ SDV PARSynthesizer (æ™‚ç³»åˆ—) ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | {datetime.now():%Y-%m-%d %H:%M:%S}")
print(f"ğŸ”© ç’°å¢ƒ: {ENVIRONMENT}")
print(f"ğŸ“ å…¥åŠ›: {INPUT_DIR}")
print(f"ğŸ’¾ å‡ºåŠ›: {OUTPUT_DIR}")

def read_csv_safe(path):
    try:
        df = pd.read_csv(path)
        print(f"âœ… {os.path.basename(path)}: {len(df):,} ä»¶, åˆ—: {df.columns.tolist()}")
        return df
    except Exception as e:
        print(f"âš ï¸ {os.path.basename(path)} ã®èª­è¾¼å¤±æ•—: {e}")
        return pd.DataFrame()

# 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
gender_df = read_csv_safe(os.path.join(INPUT_DIR, "gender.csv")) 
disease_df = read_csv_safe(os.path.join(INPUT_DIR, "disease.csv"))
inspection_df = read_csv_safe(os.path.join(INPUT_DIR, "inspection.csv"))
drug_df = read_csv_safe(os.path.join(INPUT_DIR, "drug.csv"))
emr_df = read_csv_safe(os.path.join(INPUT_DIR, "emr.csv"))

def normalize(df, date_col, type_label):
    df = df.copy()
    if date_col in df.columns:
        df["date"] = pd.to_datetime(df[date_col], errors="coerce")
    df["event_type"] = type_label
    return df

disease_df = normalize(disease_df, "disease_date", "disease")
inspection_df = normalize(inspection_df, "inspection_date", "inspection")
drug_df = normalize(drug_df, "key_date", "drug")
emr_df = normalize(emr_df, "emr_date", "emr")

# 2. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç† (PARSynthesizer ç”¨)
if "emr_text" in emr_df.columns:
    emr_df = emr_df.drop(columns=["emr_text"])

merged_df = pd.concat([disease_df, inspection_df, drug_df, emr_df], ignore_index=True)

drop_cols = [
    "inspection_value", "drug_name", "icd10_code", "disease_name",
    "yj_code", "inspection_name", "amount", "days_count",
    "extracted_number", "daily_dosage"
]
print(f"ğŸ—‘ï¸ ä»¥ä¸‹ã®é«˜ã‚³ã‚¹ãƒˆåˆ—ã‚’é™¤å¤–ã—ã¾ã™: {drop_cols}")
merged_df = merged_df.drop(columns=[c for c in drop_cols if c in merged_df.columns], errors='ignore')

merged_df["date"] = pd.to_datetime(merged_df["date"], errors="coerce")
original_count = len(merged_df)
merged_df = merged_df.dropna(subset=['date'])
print(f"ğŸ—“ï¸ æ—¥ä»˜(date)ãŒ NaT ã® {original_count - len(merged_df):,} ä»¶ã‚’é™¤å¤–ã—ã¾ã—ãŸã€‚")

date_cols = ["disease_date", "inspection_date", "key_date", "emr_date"]
merged_df = merged_df.drop(columns=[c for c in date_cols if c in merged_df.columns], errors='ignore')

if 'age' in merged_df.columns:
    merged_df['age'] = pd.to_numeric(merged_df['age'], errors='coerce').fillna(0)

cat_cols = ['event_type', 'is_suspected', 'admission_status', 'department', 'unit', 'æ¡å¦', 'emr_type', 'hospital_id']
for col in cat_cols:
    if col in merged_df.columns:
        merged_df[col] = merged_df[col].fillna('Unknown').astype('category')
            
if 'patient_id' in merged_df.columns:
    merged_df['patient_id'] = merged_df['patient_id'].astype(str)
else:
    print("âŒ è­¦å‘Š: patient_id ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚PARSynthesizer ã¯å¤±æ•—ã—ã¾ã™ã€‚")

training_data = merged_df.sort_values(by=['patient_id', 'date']).copy()

print(f"âœ… çµ±åˆã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(training_data):,}")
print("--- ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª (info) ---")
print(training_data.info())
print("--------------------------")

training_path = os.path.join(OUTPUT_DIR, f"event_training_data_sequential_{timestamp}.csv")
training_data.to_csv(training_path, index=False)
print(f"ğŸ’¾ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {training_path}")

# -----------------------------------------------------------------
# 3. PARSynthesizer (æ™‚ç³»åˆ—) ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
# -----------------------------------------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ’¡ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
print("ğŸ§  PARSynthesizer ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")

try:
    # -------------------------------------------------
    # âœ… ä¿®æ­£ç‚¹ï¼š PARãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
    # -------------------------------------------------
    metadata = SingleTableMetadata()
    metadata.detect_from_dataframe(data=training_data)
    
    # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ (èª°ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹) ã‚’æŒ‡å®š
    metadata.update_column(
        column_name='patient_id',
        sdtype='id' # 'object' (str) ã‹ã‚‰ 'id' ã«å¤‰æ›´
    )
    metadata.set_entity_columns(column_name='patient_id')

    # 2. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (ä½•é †ã‹) ã‚’æŒ‡å®š
    metadata.update_column(
        column_name='date',
        sdtype='datetime' # å¿µã®ãŸã‚å‹ã‚’æŒ‡å®š
    )
    metadata.set_sequence_index(column_name='date')
    
    print("âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®šå®Œäº†ã€‚")
    # print(metadata.to_dict()) # ãƒ‡ãƒãƒƒã‚°ç”¨ã«è¨­å®šå†…å®¹ã‚’è¡¨ç¤º

    # -------------------------------------------------
    
    print("ğŸ¤– PARSynthesizer å­¦ç¿’é–‹å§‹ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ, EPOCHS=25ï¼‰...")
    model = PARSynthesizer(
        metadata, # ğŸ‘ˆ ä¿®æ­£ç‚¹ï¼šãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
        epochs=25,
        batch_size=500,
        verbose=True,
        device_name=device
    )
    
    model.fit(training_data)
    
    model_path = os.path.join(OUTPUT_DIR, f"par_model_light_{timestamp}.pkl")
    model.save(model_path)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

except Exception as e:
    print(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
finally:
    print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
